
# üìò DOCUMENTA√á√ÉO DO PROJETO

### **Objetivo**

Automatizar o fluxo de tratamento de dados usando Apache Airflow, integrando AWS S3, Pandas e PostgreSQL.

### **Etapas do Pipeline**

1. **Extra√ß√£o (extract_data)**:
- Utiliza S3Hook para baixar arquivos .csv e .json de um bucket no S3.
- Usa par√¢metros de data (data_inicio e data_fim) para filtrar os arquivos desejados.
- Todos os arquivos s√£o salvos localmente em um diret√≥rio tempor√°rio.

2. **Transforma√ß√£o (transform_data):**
- Une todos os arquivos CSV em um √∫nico DataFrame com o pandas.concat.
- L√™ os JSONs com json.load e os transforma em DataFrame com json_normalize.
- Realiza um merge entre os dois DataFrames com base na coluna id.
- O DataFrame final √© salvo como um CSV (dados_transformados.csv).

3. **Carga (load_data):**
- L√™ o CSV final com pandas.
- Usa PostgresHook para conectar-se ao banco PostgreSQL.
- Executa um script SQL de inser√ß√£o (modelo insercao_template.sql) para cada linha do DataFrame.

4. **Limpeza (cleanup):**
- Remove todos os arquivos tempor√°rios e o diret√≥rio /tmp/etl_temp ap√≥s a conclus√£o da DAG.

**Configura√ß√µes T√©cnicas**

- **Retries**: Cada task tenta at√© 2 vezes em caso de falha, com 3 minutos de intervalo.
- **catchup=False**: Garante que execu√ß√µes anteriores √† data atual n√£o sejam processadas automaticamente.
- **schedule_interval=None**: O DAG s√≥ ser√° executado manualmente.
- **Diret√≥rios Tempor√°rios**: Isolamento dos arquivos para controle do processo e limpeza ao final.
- **M√≥dulos instalados**: As dependencias do python instaladas foram:
	- apache-airflow==2.8.1
	- pandas
	- boto3
	- psycopg2-binary


**Explica√ß√£o da Estrutura**

- A DAG foi dividida em tarefas com responsabilidades √∫nicas, seguindo o padr√£o ETL (Extract, Transform, Load). 
- Par√¢metros foram inclu√≠dos no params do DAG para flexibilizar o intervalo de dados processados.
- A l√≥gica est√° modularizada para facilitar a manuten√ß√£o e poss√≠veis melhorias (como paraleliza√ß√£o ou valida√ß√µes).

